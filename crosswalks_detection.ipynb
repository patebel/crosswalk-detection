{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd2mkkZxkyjs"
   },
   "source": [
    "##Connect Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onAWQ147lAf7"
   },
   "source": [
    "##Install Yolo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RY6uq6dSlFMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "07405454-218e-48f7-b571-bee582f0df03"
   },
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.115 ðŸš€ Python-3.11.3 torch-2.2.2 CPU (Apple M2 Max)\n",
      "Setup complete âœ… (12 CPUs, 32.0 GB RAM, 677.1/926.4 GB disk)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZSq6TNJNj7R"
   },
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2KbePJy9TkBZ",
    "ExecuteTime": {
     "end_time": "2025-04-24T16:45:23.959237Z",
     "start_time": "2025-04-24T16:45:23.841664Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "epochs = 1 #@param {type:\"slider\", min:100, max:1000, step:10}\n",
    "training_option = \"Train new CNN\" #@param [\"Train new CNN\", \"Resume an interrupted Training\"]\n",
    "weight = 'yolov8m.pt' #@param[\"yolov8n.pt\",\"yolov8s.pt\",\"yolov8m.pt\",\"yolov8l.pt\", \"yolov8x\"]\n",
    "batch = 1024 #@param {type:\"slider\", min:1, max:64, step:1}\n",
    "\n",
    "model = YOLO(weight)\n",
    "# data =  \"/content/gdrive/MyDrive/\"+folder+\"/training/training_dataset/data.yaml\"\n",
    "data =  \"training/training_dataset/data.yaml\"\n",
    "# project = \"/content/gdrive/MyDrive/\"+folder+\"/training/training_results/\"\n",
    "project = \"training/training_results\"\n",
    "# model_folder = \"/content/gdrive/MyDrive/\"+folder+\"/training/training_results/train/weights/last.pt\"\n",
    "model_folder = \"training/training_results/train/weights/last.pt\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "print(type(model))"
   ],
   "metadata": {
    "id": "fAknxsowEQvI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "80cfff84-f7ec-46bf-83e4-837ec28bee05",
    "ExecuteTime": {
     "end_time": "2025-04-24T16:45:25.673795Z",
     "start_time": "2025-04-24T16:45:25.669814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.models.yolo.model.YOLO'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nn7uK3k7lOzh",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-24T16:45:27.332358Z"
    }
   },
   "source": [
    "if training_option == 'Train new CNN':\n",
    "  model.train(data=data, epochs=epochs, imgsz=512, batch=batch, cache=True, patience=50, project=project)\n",
    "else:\n",
    "  model.train(data=data, epochs=epochs, imgsz=512, batch=batch, cache=True, patience=50, project=project, resume=True, model=model_folder)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.115 ðŸš€ Python-3.11.3 torch-2.2.2 CPU (Apple M2 Max)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0mtask=detect, mode=train, model=yolov8m.pt, data=training/training_dataset/data.yaml, epochs=1, time=None, patience=50, batch=1024, imgsz=512, save=True, save_period=-1, cache=True, device=None, workers=8, project=training/training_results, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=training/training_results/train8\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 192.2Â±165.6 MB/s, size: 57.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /Users/patebel/Code/crosswalk-detection/training/training_dataset/train/labels.cache... 1070 images, 23 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mCaching images (0.8GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [00:00<00:00, 3528.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 207.6Â±10.8 MB/s, size: 44.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /Users/patebel/Code/crosswalk-detection/training/training_dataset/valid/labels.cache... 224 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mCaching images (0.2GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:00<00:00, 4727.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to training/training_results/train8/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.008), 83 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mtraining/training_results/train8\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyESzLNjNotL"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnxYAYt7Nt2S",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "outputId": "bb2884d5-6aa8-42b5-b541-c559a9c6f0bd"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-6f76c47e8dfc>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdata_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/content/gdrive/MyDrive/\"\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mfolder\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"/test/test_dataset/data.yaml\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'val'"
     ]
    }
   ],
   "source": [
    "data_test = \"/test/test_dataset/data.yaml\"\n",
    "metrics = model.val(data=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "38-N0MWLTYyz",
    "ExecuteTime": {
     "end_time": "2025-04-24T16:42:31.539652Z",
     "start_time": "2025-04-24T16:42:31.338932Z"
    }
   },
   "source": [
    "#Printing result to confront\n",
    "from PIL import Image\n",
    "\n",
    "precision_training = '/training/training_results/train/F1_curve.png'\n",
    "precision_test = '/training/training_results/val/F1_curve.png'\n",
    "\n",
    "im1 = Image.open(precision_training)\n",
    "im2 = Image.open(precision_test)\n",
    "\n",
    "print(\"F1 Training\")\n",
    "display(im1)\n",
    "print(\"F1 Test\")\n",
    "display(im2)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/training/training_results/train/F1_curve.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m precision_training = \u001B[33m'\u001B[39m\u001B[33m/training/training_results/train/F1_curve.png\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      5\u001B[39m precision_test = \u001B[33m'\u001B[39m\u001B[33m/training/training_results/val/F1_curve.png\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m im1 = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprecision_training\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m im2 = Image.open(precision_test)\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mF1 Training\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/crosswalk-detection/.venv/lib/python3.11/site-packages/PIL/Image.py:3505\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3502\u001B[39m     filename = os.fspath(fp)\n\u001B[32m   3504\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[32m-> \u001B[39m\u001B[32m3505\u001B[39m     fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3506\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3507\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/training/training_results/train/F1_curve.png'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryt168gZZ0F6"
   },
   "source": [
    "##Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8zpVubmH90v"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Bounding Box Merge Algorithm\n",
    "def merge_bounding_boxes(bounding_boxes,image_width,image_height):\n",
    "  [x,y] = [image_height,image_width]\n",
    "  [w,h] = [0,0]\n",
    "  for i in range(0,bounding_boxes.shape[0]):\n",
    "    if(bounding_boxes.ndim == 1):\n",
    "      center_x_1 = int(bounding_boxes[0]*image_width)\n",
    "      center_y_1 = int(bounding_boxes[1]*image_height)\n",
    "      width_1 = int(bounding_boxes[2]*image_width)\n",
    "      height_1 = int(bounding_boxes[3]*image_height)\n",
    "      x = int(center_x_1 - (width_1/2))\n",
    "      y = int(center_y_1 - (height_1/2))\n",
    "      w = int(center_x_1 + (width_1/2))\n",
    "      h = int(center_y_1 + (height_1/2))\n",
    "    else:\n",
    "      center_x_1 = int(bounding_boxes[i,0]*image_width)\n",
    "      center_y_1 = int(bounding_boxes[i,1]*image_height)\n",
    "      width_1 = int(bounding_boxes[i,2]*image_width)\n",
    "      height_1 = int(bounding_boxes[i,3]*image_height)\n",
    "      vert_x = int(center_x_1 - (width_1/2))\n",
    "      vert_y = int(center_y_1 - (height_1/2))\n",
    "      vert_w = int(center_x_1 + (width_1/2))\n",
    "      vert_h = int(center_y_1 + (height_1/2))\n",
    "      if(vert_x < x):\n",
    "        x = vert_x\n",
    "      if(vert_y < y):\n",
    "        y = vert_y\n",
    "      if(vert_w > w):\n",
    "        w = vert_w\n",
    "      if(vert_h > h):\n",
    "        h = vert_h\n",
    "    return np.array([x,y,w,h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62zCqZZeZ4bC"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=\"/content/gdrive/MyDrive/$folder/training/training_results/train/weights/best.pt\" source=\"/content/gdrive/MyDrive/$folder/detection/dataset/\" imgsz=512 save=False save_txt=True save_conf=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoBgpMuXKZgc"
   },
   "outputs": [],
   "source": [
    "path = '/runs/detect/predict/labels/'\n",
    "img_path = '/detection/dataset/'\n",
    "save_path = '/runs/detect/predict/'\n",
    "\n",
    "for fname in os.listdir(save_path):\n",
    "  if fname.endswith('.jpg'):\n",
    "    (\"The path already contains images, skipping this part.\")\n",
    "    break\n",
    "else:\n",
    "  from distutils.dir_util import copy_tree\n",
    "  copy_tree(img_path, save_path)\n",
    "  print(\"Path Copied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qdEnhM7Cu-Q"
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "  with open(os.path.join(os.getcwd(),filename), 'r') as f:\n",
    "    bounding_boxes = None\n",
    "    confidences = None\n",
    "    image_name = os.path.basename(filename).replace('txt', 'jpg')\n",
    "    for line in f:\n",
    "      cl, label_x, label_y, label_w, label_h, conf = line.split(' ')\n",
    "      b = float(conf)\n",
    "      a = np.array([float(label_x),float(label_y),float(label_w),float(label_h)])\n",
    "      bounding_boxes = (np.vstack((bounding_boxes, a)) if (bounding_boxes is not None) else a)\n",
    "      confidences = (np.vstack((confidences, b)) if (confidences is not None) else b)\n",
    "    #Image Elaboration\n",
    "    image = cv.imread(img_path+image_name)\n",
    "    [image_height, image_width, levels] = image.shape\n",
    "    conf_max = np.amax(confidences)\n",
    "    [x,y,w,h] = merge_bounding_boxes(bounding_boxes, image_width, image_height)\n",
    "    #Create Rect\n",
    "    cv.rectangle(image, (x,y), (w,h), (0,0,255), 4)\n",
    "    cv.putText(image, 'crosswalk ' + \"%.2f\" % conf_max, (x, y-10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "    #Saving Image\n",
    "    print('Saving ' + image_name)\n",
    "    cv.imwrite(save_path+image_name, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUKRyqySLD1V"
   },
   "source": [
    "##Intersection Over Union (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afb7v7xELHPE",
    "outputId": "6db07246-80f6-4b50-ca66-79980bc339e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.63\n"
     ]
    }
   ],
   "source": [
    "#INTERESCTION OVER UNION (IOU)\n",
    "def compute_iou(box1, box2):\n",
    "  #DATA: box1 = [L1, T1, R1, B1], box2 = [L2, T2, R2, B2]\n",
    "  L_inter = max(box1[0], box2[0])\n",
    "  T_inter = max(box1[1], box2[1])\n",
    "  R_inter = min(box1[2], box2[2])\n",
    "  B_inter = min(box1[3], box2[3])\n",
    "  if(R_inter < L_inter) or (B_inter < T_inter):\n",
    "    return 0\n",
    "  A_inter = (R_inter - L_inter) * (B_inter - T_inter)\n",
    "  A1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "  A2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "  A_union = A1 + A2 - A_inter\n",
    "  iou = A_inter/A_union\n",
    "  return iou\n",
    "\n",
    "#READING DATA AND CALCULATING IOU\n",
    "dataset_path = '/iou/dataset/'\n",
    "detection_path = '/runs/detect/predict/labels/'\n",
    "\n",
    "iou_array = None\n",
    "\n",
    "for filename_1 in glob.glob(os.path.join(dataset_path, '*.txt')):\n",
    "  with open(os.path.join(os.getcwd(),filename_1), 'r') as f:\n",
    "    name_1 = os.path.basename(filename_1)\n",
    "    for filename_2 in glob.glob(os.path.join(detection_path, '*.txt')):\n",
    "      name_2 = os.path.basename(filename_2)\n",
    "      with open(os.path.join(os.getcwd(),filename_2), 'r') as g:\n",
    "        if(name_1 == name_2):\n",
    "          image_name = os.path.basename(filename_2).replace('txt', 'jpg')\n",
    "          bounding_boxes_1 = None\n",
    "          bounding_boxes_2 = None\n",
    "          for line in f:\n",
    "            cl_1, label_x_1, label_y_1, label_w_1, label_h_1 = line.split(' ')\n",
    "            a_1 = np.array([float(label_x_1),float(label_y_1),float(label_w_1),float(label_h_1)])\n",
    "            bounding_boxes_1 = (np.vstack((bounding_boxes_1, a_1)) if (bounding_boxes_1 is not None) else a_1)\n",
    "          for line in g:\n",
    "            cl_2, label_x_2, label_y_2, label_w_2, label_h_2, conf_2 = line.split(' ')\n",
    "            a_2 = np.array([float(label_x_2),float(label_y_2),float(label_w_2),float(label_h_2)])\n",
    "            bounding_boxes_2 = (np.vstack((bounding_boxes_2, a_2)) if (bounding_boxes_2 is not None) else a_2)\n",
    "          image = cv.imread(dataset_path+image_name)\n",
    "          [image_height, image_width, levels] = image.shape\n",
    "          merge_box_1 = merge_bounding_boxes(bounding_boxes_1, image_width, image_height)\n",
    "          merge_box_2 = merge_bounding_boxes(bounding_boxes_2, image_width, image_height)\n",
    "          iou = compute_iou(merge_box_1, merge_box_2)\n",
    "          iou_array = (np.vstack((iou_array, iou)) if (iou_array is not None) else iou)\n",
    "        else:\n",
    "          continue\n",
    "print('IoU: ' + str(iou_array.mean())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKvv_Q_LNiaF"
   },
   "source": [
    "##Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6SbK_OENlfF"
   },
   "outputs": [],
   "source": [
    "video = 'video.mp4' #@param {type:\"string\"}\n",
    "!yolo task=detect mode=predict model=\"/content/gdrive/MyDrive/$folder/training/training_results/train/weights/best.pt\" source=\"/content/gdrive/MyDrive/$folder/detection/video/$video\" imgsz=512 save_txt=True save_conf=True project=\"video_results/\" save=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSpyabebMV_Y"
   },
   "outputs": [],
   "source": [
    "#CREATING INFERENCE ON VIDEO\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "path = '/video_results/predict/labels/'\n",
    "video_path = '/detection/video/'\n",
    "save_path = '/video_results/'\n",
    "frames_path = '/video_results/predict/frames/'\n",
    "video_name = video.replace('.mp4', '')\n",
    "\n",
    "print('Reading Video: '+video_path+video)\n",
    "v = cv.VideoCapture(video_path+video)\n",
    "frame_count = int(v.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "fps = v.get(cv.CAP_PROP_FPS)\n",
    "video_w = int(v.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "video_h = int(v.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "print('Video Info' + '  FPS: '+str(int(fps))+' Width: '+str(video_w)+' Height: '+str(video_h))\n",
    "\n",
    "frames = None\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "  with open(os.path.join(os.getcwd(),filename), 'r') as f:\n",
    "    bounding_boxes = None\n",
    "    confidences = None\n",
    "    #Saves the Frame number in an array\n",
    "    frame_num = int(os.path.basename(filename).replace('.txt', '').replace(video_name +'_',''))\n",
    "    frames = (np.vstack((frames, frame_num)) if (frames is not None) else frame_num)\n",
    "    for line in f:\n",
    "      cl, label_x, label_y, label_w, label_h, conf = line.split(' ')\n",
    "      b = float(conf)\n",
    "      a = np.array([float(label_x),float(label_y),float(label_w),float(label_h)])\n",
    "      bounding_boxes = (np.vstack((bounding_boxes, a)) if (bounding_boxes is not None) else a)\n",
    "      confidences = (np.vstack((confidences, b)) if (confidences is not None) else b)\n",
    "    conf_max = np.amax(confidences)\n",
    "    #Get Frames from Video\n",
    "    v.set(cv.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    res, image = v.read()\n",
    "    #cv2_imshow(image)\n",
    "    image_height = video_h\n",
    "    image_width = video_w\n",
    "    [x,y,w,h] = merge_bounding_boxes(bounding_boxes, image_width, image_height)\n",
    "    cv.rectangle(image, (x,y), (w,h), (0,0,255), 4)\n",
    "    cv.putText(image, 'crosswalk ' + \"%.2f\" % conf_max, (x, y+20), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "    if image is None:\n",
    "      continue\n",
    "    cv.imwrite(frames_path+str(frame_num)+'.jpg', image)\n",
    "v.release()\n",
    "print(\"Frame Creation Complete. Run the following code to create video and clear frames from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUky9PdtRGaQ"
   },
   "outputs": [],
   "source": [
    "#SAVING NEW VIDEO INFERENCES\n",
    "cap = cv.VideoCapture(video_path+video)\n",
    "vid_writer = cv.VideoWriter(save_path+video, cv.VideoWriter_fourcc(*'mp4v'), fps, (video_w, video_h))\n",
    "frame_num = -1;\n",
    "while (cap.isOpened()):\n",
    "  frame_num +=1\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    if(frame_num in frames):\n",
    "      frame = cv.imread(frames_path+str(frame_num)+'.jpg')\n",
    "    vid_writer.write(frame)\n",
    "  else:\n",
    "    break\n",
    "cap.release()\n",
    "vid_writer.release()\n",
    "\n",
    "shutil.rmtree(frames_path)\n",
    "print('Result Saved on '+save_path+video)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Export in ONNX"
   ],
   "metadata": {
    "id": "iWpEmMVpJbgi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dewZ-dUkmIuu"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "#model = YOLO(\"/content/gdrive/MyDrive/Crosswalks_Detection/training/training_results/train/weights/best.pt\")\n",
    "#model.export(format=\"onnx\")\n",
    "!yolo export model=\"/training/training_results/train/weights/best.pt\" format=onnx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cyESzLNjNotL",
    "Ryt168gZZ0F6",
    "kUKRyqySLD1V",
    "hKvv_Q_LNiaF",
    "iWpEmMVpJbgi"
   ],
   "provenance": [],
   "authorship_tag": "ABX9TyMyBmbaEyZlKYqu+oDaihDb"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
